<!DOCTYPE html>
<html lang="en">
<head>
        <meta charset="utf-8" />
        <title>Full tensors in Torchscript</title>
        <link rel="stylesheet" href="https://blog.tedwild.dev/theme/css/main.css" />

        <!--[if IE]>
            <script src="https://html5shiv.googlecode.com/svn/trunk/html5.js"></script>
        <![endif]-->
</head>

<body id="index" class="home">
        <header id="banner" class="body">
                <h1><a href="https://blog.tedwild.dev/">Ted Wild's Blog </a></h1>
                <nav><ul>
                    <li><a href="https://blog.tedwild.dev/category/anki-tips.html">anki, tips</a></li>
                    <li><a href="https://blog.tedwild.dev/category/tips.html">Tips</a></li>
                    <li class="active"><a href="https://blog.tedwild.dev/category/tips-torchscript.html">tips, torchscript</a></li>
                    <li><a href="https://blog.tedwild.dev/category/vanity-vim.html">vanity, vim</a></li>
                </ul></nav>
        </header><!-- /#banner -->
<section id="content" class="body">
  <article>
    <header>
      <h1 class="entry-title">
        <a href="https://blog.tedwild.dev/full-tensors-in-torchscript.html" rel="bookmark"
           title="Permalink to Full tensors in Torchscript">Full tensors in Torchscript</a></h1>
    </header>

    <div class="entry-content">
<footer class="post-info">
        <abbr class="published" title="2020-02-08T13:30:00-08:00">
                Published: Sat 08 February 2020
        </abbr>

        <address class="vcard author">
                By                         <a class="url fn" href="https://blog.tedwild.dev/author/ted.html">Ted</a>
        </address>
<p>In <a href="https://blog.tedwild.dev/category/tips-torchscript.html">tips, torchscript</a>.</p>

</footer><!-- /.post-info -->      <h1>Full tensors in Torchscript</h1>
<p>I was productizing some PyTorch models recently and ran in to an issue with the way <code>torch.full_like</code> behaves in Torchscript.</p>
<div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">torch</span>
</pre></div>


<p>The original code use <code>torch.full_like</code> to create a tensor with a given shape.  It also forced the type of the new tensor to <code>torch.float16</code>.  A minimal example that illustrates the issue is below.</p>
<div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">full</span><span class="p">(</span><span class="n">val</span><span class="p">:</span> <span class="nb">float</span><span class="p">,</span> <span class="n">base</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">full_like</span><span class="p">(</span><span class="n">base</span><span class="p">,</span> <span class="n">val</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float16</span><span class="p">)</span>
<span class="n">full</span><span class="p">(</span><span class="mf">42.0</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">]))</span>
</pre></div>


<div class="highlight"><pre><span></span>tensor([42., 42.], dtype=torch.float16)
</pre></div>


<p>Converting directly to Torchscript fails, apparently because Torchscript can't select the appropriate overload.</p>
<div class="highlight"><pre><span></span><span class="nd">@torch.jit.script</span>
<span class="k">def</span> <span class="nf">full1</span><span class="p">(</span><span class="n">val</span><span class="p">:</span> <span class="nb">float</span><span class="p">,</span> <span class="n">base</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">full_like</span><span class="p">(</span><span class="n">base</span><span class="p">,</span> <span class="n">val</span><span class="p">,</span> <span class="n">dytpe</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float16</span><span class="p">)</span>

<span class="n">full1</span><span class="p">(</span><span class="mf">42.0</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">]))</span>
</pre></div>


<p>...</p>
<div class="highlight"><pre><span></span><span class="n">RuntimeError</span><span class="o">:</span> 
<span class="n">Arguments</span> <span class="k">for</span> <span class="n">call</span> <span class="n">are</span> <span class="n">not</span> <span class="n">valid</span><span class="o">.</span>
<span class="n">The</span> <span class="n">following</span> <span class="n">variants</span> <span class="n">are</span> <span class="n">available</span><span class="o">:</span>

  <span class="n">aten</span><span class="o">::</span><span class="n">full_like</span><span class="o">(</span><span class="n">Tensor</span> <span class="n">self</span><span class="o">,</span> <span class="n">Scalar</span> <span class="n">fill_value</span><span class="o">,</span> <span class="o">*,</span> <span class="n">int</span><span class="o">?</span> <span class="n">memory_format</span><span class="o">=</span><span class="n">None</span><span class="o">)</span> <span class="o">-&amp;</span><span class="n">gt</span><span class="o">;</span> <span class="o">(</span><span class="n">Tensor</span><span class="o">):</span>
  <span class="n">Keyword</span> <span class="n">argument</span> <span class="n">dytpe</span> <span class="n">unknown</span><span class="o">.</span>

  <span class="n">aten</span><span class="o">::</span><span class="n">full_like</span><span class="o">.</span><span class="na">dtype</span><span class="o">(</span><span class="n">Tensor</span> <span class="n">self</span><span class="o">,</span> <span class="n">Scalar</span> <span class="n">fill_value</span><span class="o">,</span> <span class="o">*,</span> <span class="n">int</span> <span class="n">dtype</span><span class="o">,</span> <span class="n">int</span> <span class="n">layout</span><span class="o">,</span> <span class="n">Device</span> <span class="n">device</span><span class="o">,</span> <span class="n">bool</span> <span class="n">pin_memory</span><span class="o">=</span><span class="n">False</span><span class="o">,</span> <span class="n">int</span><span class="o">?</span> <span class="n">memory_format</span><span class="o">=</span><span class="n">None</span><span class="o">)</span> <span class="o">-&amp;</span><span class="n">gt</span><span class="o">;</span> <span class="o">(</span><span class="n">Tensor</span><span class="o">):</span>
  <span class="n">Argument</span> <span class="n">dtype</span> <span class="n">not</span> <span class="n">provided</span><span class="o">.</span>

<span class="n">The</span> <span class="n">original</span> <span class="n">call</span> <span class="k">is</span><span class="o">:</span>
  <span class="n">File</span> <span class="s2">&quot;&amp;lt;ipython-input-8-d96dada73816&amp;gt;&quot;</span><span class="o">,</span> <span class="n">line</span> <span class="mi">3</span>
<span class="err">@</span><span class="n">torch</span><span class="o">.</span><span class="na">jit</span><span class="o">.</span><span class="na">script</span>
<span class="n">def</span> <span class="n">full1</span><span class="o">(</span><span class="n">val</span><span class="o">:</span> <span class="n">float</span><span class="o">,</span> <span class="n">base</span><span class="o">:</span> <span class="n">torch</span><span class="o">.</span><span class="na">Tensor</span><span class="o">):</span>
    <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="na">full_like</span><span class="o">(</span><span class="n">base</span><span class="o">,</span> <span class="n">val</span><span class="o">,</span> <span class="n">dytpe</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="na">float16</span><span class="o">)</span>
           <span class="o">~~~~~~~~~~~~~~~</span> <span class="o">&amp;</span><span class="n">lt</span><span class="o">;---</span> <span class="n">HERE</span>
</pre></div>


<h1>Workarounds</h1>
<h2>Provide all the arguments</h2>
<p>The obvious solution is to provide all the arguments.  That works, but is rather more verbose.</p>
<div class="highlight"><pre><span></span><span class="nd">@torch.jit.script</span>
<span class="k">def</span> <span class="nf">full2</span><span class="p">(</span><span class="n">val</span><span class="p">:</span> <span class="nb">float</span><span class="p">,</span> <span class="n">base</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">full_like</span><span class="p">(</span>
        <span class="n">base</span><span class="p">,</span>
        <span class="n">val</span><span class="p">,</span>
        <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float16</span><span class="p">,</span>
        <span class="n">layout</span><span class="o">=</span><span class="n">base</span><span class="o">.</span><span class="n">layout</span><span class="p">,</span>
        <span class="n">device</span><span class="o">=</span><span class="n">base</span><span class="o">.</span><span class="n">device</span><span class="p">,</span>
    <span class="p">)</span>

<span class="n">full2</span><span class="p">(</span><span class="mf">42.0</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">]))</span>
</pre></div>


<div class="highlight"><pre><span></span>tensor([42., 42.], dtype=torch.float16)
</pre></div>


<h2>Switch to torch.full</h2>
<p>In this case, only the shape is needed.  The device and layout are already the defaults, so <code>torch.full</code> is appropriate.</p>
<div class="highlight"><pre><span></span><span class="nd">@torch.jit.script</span>
<span class="k">def</span> <span class="nf">full3</span><span class="p">(</span><span class="n">val</span><span class="p">:</span> <span class="nb">float</span><span class="p">,</span> <span class="n">base</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">full</span><span class="p">(</span><span class="n">base</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">val</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float16</span><span class="p">)</span>

<span class="n">full3</span><span class="p">(</span><span class="mf">42.0</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">]))</span>
</pre></div>


<div class="highlight"><pre><span></span>tensor([42., 42.], dtype=torch.float16)
</pre></div>
    </div><!-- /.entry-content -->

  </article>
</section>
        <section id="extras" class="body">
                <div class="blogroll">
                        <h2>links</h2>
                        <ul>
                            <li><a href="https://blog.tedwild.dev/extras/tedwild.pdf">Résumé</a></li>
                            <li><a href="http://pages.cs.wisc.edu/~wildt/">My old page</a></li>
                        </ul>
                </div><!-- /.blogroll -->
                <div class="social">
                        <h2>social</h2>
                        <ul>

                            <li><a href="https://www.linkedin.com/in/ted-wild">LinkedIn</a></li>
                            <li><a href="https://github.com/twwhatever">GitHub</a></li>
                        </ul>
                </div><!-- /.social -->
        </section><!-- /#extras -->

        <footer id="contentinfo" class="body">
                <address id="about" class="vcard body">
                Proudly powered by <a href="http://getpelican.com/">Pelican</a>, which takes great advantage of <a href="http://python.org">Python</a>.
                </address><!-- /#about -->

                <p>The theme is by <a href="http://coding.smashingmagazine.com/2009/08/04/designing-a-html-5-layout-from-scratch/">Smashing Magazine</a>, thanks!</p>
        </footer><!-- /#contentinfo -->

</body>
</html>